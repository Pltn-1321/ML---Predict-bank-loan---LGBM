# ğŸ¦ ModÃ¨le de Scoring de CrÃ©dit - Projet MLOps OpenClassrooms

## Formation AI Engineer 2026 - Projet OC6

[![MLFlow](https://img.shields.io/badge/MLFlow-Tracking-blue.svg)](https://mlflow.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-API-009688.svg)](https://fastapi.tiangolo.com/)
[![Python](https://img.shields.io/badge/Python-3.11+-green.svg)](https://www.python.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit-learn-ML-orange.svg)](https://scikit-learn.org/)
[![LightGBM](https://img.shields.io/badge/LightGBM-Boosting-yellow.svg)](https://lightgbm.readthedocs.io/)
[![Docker](https://img.shields.io/badge/Docker-Container-2496ED.svg)](https://www.docker.com/)
[![CI/CD](https://img.shields.io/badge/GitHub_Actions-CI%2FCD-2088FF.svg)](https://github.com/features/actions)

### ğŸ“Š **RÃ©sumÃ© ExÃ©cutif**

**ProblÃ¨me mÃ©tier** : PrÃ©dire le risque de dÃ©faut de paiement des clients d'une institution financiÃ¨re de microcrÃ©dit (Home Credit Default Risk).

**DÃ©fi principal** : Dataset massivement dÃ©sÃ©quilibrÃ© (91.9% bons clients vs 8.1% dÃ©fauts â†’ ratio **11.4:1**) + 8 tables relationnelles Ã  agrÃ©ger.

**Solution proposÃ©e** : Pipeline MLOps complet, de l'entraÃ®nement au dÃ©ploiement :

- AgrÃ©gation hiÃ©rarchique de 57M+ lignes â†’ 305 features
- **Feature "Has_History"** : capture l'absence d'historique (info critique)
- **Imputation stratÃ©gique** : 5 approches selon sÃ©mantique mÃ©tier
- **Score mÃ©tier personnalisÃ©** : FN = 10Ã— FP (prioritÃ© recall)
- **MLFlow tracking complet** : baselines, tuning, seuil optimal
- **API FastAPI** de scoring en production avec monitoring Streamlit
- **CI/CD GitHub Actions** : tests automatisÃ©s, build Docker, dÃ©ploiement Render

**RÃ©sultats** : ModÃ¨le LightGBM (Val AUC = 0.7852, Business Cost = 0.4907), seuil optimal 0.494, dÃ©ployÃ© via API REST avec dashboard de monitoring.

---

## ğŸ¯ **Objectifs du Projet**

### Partie 1 â€” ModÃ©lisation
1. **IngÃ©nierie des features avancÃ©es** Ã  partir de donnÃ©es relationnelles complexes
2. **Pipeline preprocessing robuste** gÃ©rant intelligemment les NaN mÃ©tier
3. **ModÃ©lisation orientÃ©e business** avec score coÃ»t asymÃ©trique (FN=10, FP=1)
4. **MLOps** : tracking expÃ©rimentations, reproductibilitÃ©, model registry
5. **Optimisation du seuil de dÃ©cision** pour minimiser le coÃ»t mÃ©tier

### Partie 2 â€” DÃ©ploiement
6. **API REST** de scoring via FastAPI
7. **Tests unitaires** automatisÃ©s (pytest, 19 tests)
8. **Dashboard de monitoring** Streamlit (scores, latence, data drift)
9. **Containerisation Docker** pour la production
10. **Pipeline CI/CD** GitHub Actions (test â†’ build â†’ deploy sur Render)

---

## ğŸ—ï¸ **Architecture du Pipeline MLOps**

```
ğŸ“¥ DonnÃ©es Brutes (8 CSV, 57M+ lignes)
    â†“ AgrÃ©gation HiÃ©rarchique (Notebook 01)
ğŸ“Š train_aggregated.csv (307k Ã— 305 features)
    â†“ Preprocessing + Feature Engineering (Notebook 02)
âš™ï¸ train_preprocessed.csv (307k Ã— 419 features, 0 NaN, scalÃ©)
    â†“ Modeling + MLFlow (Notebook 03)
ğŸš€ Meilleur ModÃ¨le LightGBM (trackÃ© MLFlow, seuil 0.494)
    â†“ Export modÃ¨le (scripts/export_model.py)
ğŸ“¦ artifacts/ (model.pkl, scaler.pkl, feature_names.json)
    â†“ API FastAPI + Docker
ğŸŒ API REST /predict â†’ probabilitÃ© + dÃ©cision (APPROVED/REFUSED)
    â†“ CI/CD GitHub Actions
â˜ï¸ DÃ©ploiement automatique sur Render
```

---

## ğŸ“ **Structure du Projet**

```
OC6_MLOPS/
â”œâ”€â”€ api/                           # API de scoring (FastAPI)
â”‚   â”œâ”€â”€ app.py                     # Routes (/health, /predict, /model-info)
â”‚   â”œâ”€â”€ predict.py                 # Chargement modÃ¨le + infÃ©rence
â”‚   â”œâ”€â”€ schemas.py                 # SchÃ©mas Pydantic request/response
â”‚   â””â”€â”€ config.py                  # Configuration (seuil, chemins)
â”œâ”€â”€ artifacts/                     # ModÃ¨le exportÃ© (commitÃ© dans git)
â”‚   â”œâ”€â”€ model.pkl                  # LightGBM (joblib)
â”‚   â”œâ”€â”€ scaler.pkl                 # StandardScaler
â”‚   â”œâ”€â”€ feature_names.json         # 419 features attendues
â”‚   â””â”€â”€ model_metadata.json        # Seuil, coÃ»ts, metadata
â”œâ”€â”€ monitoring/                    # Dashboard Streamlit + drift
â”‚   â”œâ”€â”€ dashboard.py               # Dashboard 5 onglets (prediction, scores, latence, drift, modÃ¨le)
â”‚   â”œâ”€â”€ drift.py                   # Simulation drift + KS test
â”‚   â””â”€â”€ predictions_log.jsonl      # Log des prÃ©dictions (JSON Lines, gÃ©nÃ©rÃ© par l'API)
â”œâ”€â”€ tests/                         # Tests unitaires (pytest, 19 tests)
â”‚   â”œâ”€â”€ test_api.py                # Tests endpoints API (7 tests)
â”‚   â”œâ”€â”€ test_predict.py            # Tests logique de prÃ©diction (4 tests)
â”‚   â””â”€â”€ test_drift.py              # Tests dÃ©tection de drift (8 tests)
â”œâ”€â”€ src/                           # Code modulaire rÃ©utilisable
â”‚   â”œâ”€â”€ data_processing.py         # Alignement features
â”‚   â””â”€â”€ metrics.py                 # Score mÃ©tier (FN=10, FP=1)
â”œâ”€â”€ notebooks/                     # Pipeline en 3 Ã©tapes
â”‚   â”œâ”€â”€ 01_EDA.ipynb               # EDA + AgrÃ©gation
â”‚   â”œâ”€â”€ 02_preprocessing_and_feature_engineering.ipynb
â”‚   â””â”€â”€ 03_modeling_with_MLFLOW.ipynb
â”œâ”€â”€ scripts/                       # Scripts utilitaires
â”‚   â”œâ”€â”€ export_model.py            # Export depuis MLflow â†’ artifacts/
â”‚   â””â”€â”€ generate_sample_predictions.py  # GÃ©nÃ¨re des prÃ©dictions de dÃ©mo (500 lignes)
â”œâ”€â”€ .github/workflows/ci-cd.yml   # GitHub Actions (test â†’ build â†’ deploy)
â”œâ”€â”€ Dockerfile                     # Image Docker production
â”œâ”€â”€ docker-compose.yml             # API + Dashboard local
â”œâ”€â”€ main.py                        # Point d'entrÃ©e uvicorn
â”œâ”€â”€ pyproject.toml                 # DÃ©pendances (uv)
â””â”€â”€ README.md
```

---

## ğŸš€ **Installation & ExÃ©cution**

### PrÃ©requis

- Python 3.11+
- [uv](https://docs.astral.sh/uv/) (recommandÃ©) ou pip

### Installation

```bash
git clone <votre-repo>
cd OC6_MLOPS
uv sync          # ou pip install -e .
```

### Lancer l'API

```bash
uv run python main.py
# API disponible sur http://localhost:8000
# Documentation Swagger : http://localhost:8000/docs
```

### Lancer les tests

```bash
uv run pytest tests/ -v
```

### Lancer le dashboard de monitoring

```bash
uv run streamlit run monitoring/dashboard.py
# Dashboard disponible sur http://localhost:8501
```

### Lancer avec Docker

```bash
# API seule
docker build -t credit-scoring .
docker run -p 8000:8000 credit-scoring

# API + Dashboard
docker compose up
```

### Lancer le pipeline notebooks

```bash
jupyter notebook notebooks/01_EDA.ipynb
jupyter notebook notebooks/02_preprocessing_and_feature_engineering.ipynb
jupyter notebook notebooks/03_modeling_with_MLFLOW.ipynb
mlflow ui   # http://localhost:5000
```

---

## ğŸ“ **MÃ©thodologie DÃ©taillÃ©e par Notebook**

### **Notebook 01 : EDA & AgrÃ©gation HiÃ©rarchique** ğŸ”

**Objectifs** :

- Charger 8 tables (57M+ lignes total)
- Analyser relations : `application_train â† bureau â† bureau_balance`, `previous_application â† POS/CC/Installments`
- CrÃ©er dataset plat pour ML

**Innovations** :

- **AgrÃ©gation en cascade** : `bureau_balance` (27M) â†’ `bureau` â†’ client
- 183 features crÃ©Ã©es : 45 bureau + 138 previous_application
- **Statistiques riches** : min/max/mean/sum + one-hot catÃ©gorielles
- **Visualisations avancÃ©es** : 5 graphiques EDA (Ã¢ge, corrÃ©lations, EXT_SOURCE, ratios, bureau)

**RÃ©sultats** :

```
307,511 clients Ã— 305 features
DÃ©sÃ©quilibre : 91.9% bons vs 8.1% dÃ©fauts (11.4:1)
250/305 colonnes NaN (normal : absence historique)
Outputs : train_aggregated.csv + test_aggregated.csv
```

### **Notebook 02 : Preprocessing & Feature Engineering AvancÃ©** âš™ï¸

**Objectifs** :

- GÃ©rer 250 colonnes NaN intelligemment
- CrÃ©er features mÃ©tier prÃ©dictives
- PrÃ©parer donnÃ©es scalÃ©es pour ML

**Innovations ClÃ©s** :

1. **Feature "Has_History"** :

   ```
   HAS_BUREAU, HAS_PREV_APP, HAS_CREDIT_CARD, HAS_POS_CASH, HAS_INSTALLMENTS
   CrÃ©Ã©es AVANT imputation â†’ capture "aucun historique = info mÃ©tier"
   ```

2. **Imputation StratÃ©gique (5 rÃ¨gles sÃ©mantiques)** :
   | Type Colonne | StratÃ©gie | Exemple | Rationale |
   |------------------|---------------|--------------------------|-----------|
   | Montants (AMT*) | 0 | AMT_CREDIT_SUM â†’ 0 | Pas de crÃ©dit = 0â‚¬ |
   | Comptages (CNT*) | 0 | SK_ID_BUREAU_COUNT â†’ 0 | 0 occurrence |
   | Dates (DAYS*) | -999 | DAYS_BIRTH â†’ -999 | Sentinelle |
   | Moyennes (MEAN*) | MÃ©diane | EXT_SOURCE_MEAN â†’ median | Robuste outliers |
   | Autres | MÃ©diane | - | DÃ©faut conservateur |

3. **Feature Engineering MÃ©tier (11 nouvelles)** :
   - CREDIT_INCOME_RATIO, ANNUITY_INCOME_RATIO
   - AGE_YEARS, EMPLOYMENT_YEARS
   - EXT_SOURCE_MEAN, EXT_SOURCE_PROD
   - INCOME_PER_PERSON, CHILDREN_RATIO
   - BUREAU_DEBT_INCOME_RATIO

**RÃ©sultats** :

```
307k Ã— 419 features | 0 NaN | 0 Inf | ScalÃ© (mean=0, std=1)
Scaler.pkl sauvegardÃ© (production-ready)
```

### **Notebook 03 : Modeling MLOps avec MLFlow** ğŸ¯

**Objectifs** :

- Baselines + tuning avec tracking MLFlow
- Score mÃ©tier asymÃ©trique (FN=10, FP=1)
- Optimisation du seuil de dÃ©cision

**Approche** :

1. **Score MÃ©tier PersonnalisÃ©** :

   ```python
   coÃ»t_total = (FN Ã— 10) + FP    # Recall prioritaire
   ```

2. **5 Baselines ComparÃ©es** :
   | ModÃ¨le | Avantages | CV Business Cost |
   |------------------|------------------------|------------------|
   | Logistic Reg (balanced) | LinÃ©aire, rapide | Baseline |
   | Logistic Reg (non-balanced) | RÃ©fÃ©rence | Pire |
   | Random Forest | Non-linÃ©aire | Moyen |
   | XGBoost | Gradient Boosting | Bon |
   | **LightGBM** | **Gradient Boosting, rapide** | **Meilleur** |

3. **Hyperparameter Tuning** : GridSearchCV sur LightGBM
4. **Seuil Optimal** : 0.494 (vs 0.5 dÃ©faut) â†’ minimise le coÃ»t mÃ©tier
5. **Ã‰valuation sur validation set** : AUC = 0.7852, Business Cost = 0.4907
6. **MLFlow Complet** : paramÃ¨tres, mÃ©triques, matrices confusion, modÃ¨les loggÃ©s, model registry

---

## ğŸ“Š **MÃ©triques ClÃ©s**

```
Dataset : 307k train | 48k test | 11.4:1 imbalance
Features : 122 orig â†’ 305 agrÃ©gÃ©es â†’ 419 finales
Meilleur ModÃ¨le : LightGBM Tuned
Seuil Optimal : 0.494 (vs 0.5 dÃ©faut)
Val AUC : 0.7852
Val Business Cost : 0.4907
Tests : 19/19 passent
```

---

## ğŸŒ **API de Scoring**

L'API FastAPI expose le modÃ¨le en production :

| Endpoint | MÃ©thode | Description |
|-----------|---------|-------------|
| `/health` | GET | Status de l'API + modÃ¨le chargÃ© |
| `/predict` | POST | PrÃ©diction de scoring (proba + dÃ©cision) |
| `/model-info` | GET | Metadata du modÃ¨le |

**Exemple de requÃªte :**

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"SK_ID_CURR": 100001, "features": {"AMT_CREDIT": 0.5, "AMT_ANNUITY": -0.3}}'
```

**RÃ©ponse :**
```json
{
  "SK_ID_CURR": 100001,
  "probability": 0.38,
  "prediction": 0,
  "threshold": 0.494,
  "decision": "APPROVED",
  "inference_time_ms": 2.5
}
```

---

## ğŸ“ˆ **Dashboard Monitoring**

Dashboard Streamlit avec 5 onglets :

1. **Prediction** â€” Scoring client interactif (par ID ou saisie manuelle de features)
2. **Scores & DÃ©cisions** â€” Distribution des probabilitÃ©s, taux de refus, rÃ©partition approuvÃ©s/refusÃ©s
3. **Performance API** â€” Latence (P50, P95, max), Ã©volution temporelle
4. **Data Drift** â€” Simulation de drift (graduel/soudain/feature shift), test KS par feature, rapport Evidently AI
5. **ModÃ¨le** â€” Metadata, seuil optimal, coÃ»ts mÃ©tier, configuration complÃ¨te

### Log des prÃ©dictions (`monitoring/predictions_log.jsonl`)

Chaque appel Ã  `/predict` est enregistrÃ© au format **JSON Lines** dans `monitoring/predictions_log.jsonl`. Chaque ligne contient :

| Champ | Description |
|-------|-------------|
| `timestamp` | Horodatage UTC (ISO 8601) |
| `SK_ID_CURR` | Identifiant client |
| `probability` | ProbabilitÃ© de dÃ©faut (0-1) |
| `prediction` | DÃ©cision binaire (0=approved, 1=refused) |
| `inference_time_ms` | Temps d'infÃ©rence du modÃ¨le en ms |

Ce fichier alimente les onglets **Scores & DÃ©cisions** et **Performance API** du dashboard.

### GÃ©nÃ©rer des donnÃ©es de dÃ©mo

Pour tester le dashboard sans lancer l'API, un script gÃ©nÃ¨re 500 prÃ©dictions rÃ©alistes :

```bash
uv run python scripts/generate_sample_predictions.py
```

Le script simule une distribution bimodale (92% bons clients, 8% dÃ©fauts) avec des timestamps rÃ©partis sur 48h et des latences rÃ©alistes (~3ms).

---

## ğŸ”„ **CI/CD**

Pipeline GitHub Actions en 3 Ã©tapes :
1. **Test** â€” `ruff check` (linting) + `pytest` (19 tests unitaires)
2. **Build** â€” `docker build` + test `/health` dans le container
3. **Deploy** â€” DÃ©ploiement automatique sur Render (push main uniquement)

---

## ğŸ’¡ **Points Forts MÃ©thodologiques**

| Innovation                | Impact MÃ©tier/Business                     |
| ------------------------- | ------------------------------------------ |
| **Has_History features**  | "Nouveau client" = risque â†’ info critique  |
| **Imputation sÃ©mantique** | Respecte logique bancaire (0â‚¬=pas crÃ©dit)  |
| **Score FN=10Ã—FP**        | Recall prioritaire (perte >> manque gain)  |
| **Seuil 0.494**           | Minimise le coÃ»t mÃ©tier vs 0.5 par dÃ©faut  |
| **No Data Leakage**       | Scaler fit train only                      |
| **MLFlow end-to-end**     | Reproductible, auditable, production-ready |
| **API + monitoring**      | ModÃ¨le dÃ©ployÃ© avec suivi en production    |
| **CI/CD automatisÃ©**      | Tests + build + deploy Ã  chaque push       |

---

## ğŸ‘¨â€ğŸ’» **Auteur & Licence**

**Auteur** : Pierre Pluton
**Formation** : OpenClassrooms AI Engineer 2026 - Projet OC6 MLOps
**Date** : FÃ©vrier 2026

**Licence** : MIT License

```
Â© 2026 Pierre Pluton.
```

---

**Contact** : pierre.pluton@outlook.fr | pierre@thoughtside.com
